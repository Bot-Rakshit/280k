{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chess Model Training - Qwen2.5-1.5B-Instruct\n",
    "\n",
    "Fine-tune Qwen2.5-1.5B-Instruct on ~280K chess positions for move prediction.\n",
    "\n",
    "**Hardware**: A40/A100 GPU\n",
    "\n",
    "**Training**: Full fine-tuning (recommended for best quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch transformers datasets accelerate bitsandbytes\n",
    "!pip install -q trl peft wandb hf_transfer\n",
    "!pip install -q flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Model\n",
    "BASE_MODEL = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "OUTPUT_DIR = \"./qwen-chess-1.5b-v1\"\n",
    "\n",
    "# Data paths - split into 2 files to stay under GitHub 100MB limit\n",
    "DATA_PART1 = \"train_data_part1.jsonl\"\n",
    "DATA_PART2 = \"train_data_part2.jsonl\"\n",
    "\n",
    "# Training mode: \"full\" or \"lora\"\n",
    "TRAINING_MODE = \"full\"  # Full fine-tuning for best quality\n",
    "\n",
    "# Hyperparameters\n",
    "CONFIG = {\n",
    "    # Training\n",
    "    \"num_epochs\": 3,\n",
    "    \"batch_size\": 4,\n",
    "    \"gradient_accumulation\": 8,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"warmup_ratio\": 0.03,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"max_seq_length\": 512,\n",
    "    \n",
    "    # LoRA specific (only if TRAINING_MODE=\"lora\")\n",
    "    \"lora_r\": 64,\n",
    "    \"lora_alpha\": 128,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \n",
    "    # Optimization\n",
    "    \"use_flash_attention\": True,\n",
    "    \"bf16\": True,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \n",
    "    # Logging\n",
    "    \"logging_steps\": 50,\n",
    "    \"save_steps\": 1000,\n",
    "    \"eval_steps\": 500,\n",
    "    \"use_wandb\": False,\n",
    "}\n",
    "\n",
    "print(f\"Training mode: {TRAINING_MODE}\")\n",
    "print(f\"Effective batch size: {CONFIG['batch_size'] * CONFIG['gradient_accumulation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(filepath):\n",
    "    \"\"\"Load JSONL file.\"\"\"\n",
    "    data = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return data\n",
    "\n",
    "# Load and merge data parts\n",
    "print(\"Loading data...\")\n",
    "part1 = load_jsonl(DATA_PART1)\n",
    "part2 = load_jsonl(DATA_PART2)\n",
    "\n",
    "print(f\"Part 1: {len(part1):,}\")\n",
    "print(f\"Part 2: {len(part2):,}\")\n",
    "\n",
    "# Combine\n",
    "all_data = part1 + part2\n",
    "print(f\"Total: {len(all_data):,}\")\n",
    "\n",
    "# Shuffle\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview a sample\n",
    "sample = all_data[0]\n",
    "print(\"Sample training example:\")\n",
    "print(\"=\"*60)\n",
    "print(\"USER:\")\n",
    "print(sample[\"messages\"][0][\"content\"][:500])\n",
    "print(\"\\nASSISTANT:\")\n",
    "print(sample[\"messages\"][1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/eval split\n",
    "train_size = int(len(all_data) * 0.98)\n",
    "train_data = all_data[:train_size]\n",
    "eval_data = all_data[train_size:]\n",
    "\n",
    "print(f\"Train: {len(train_data):,}\")\n",
    "print(f\"Eval: {len(eval_data):,}\")\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "eval_dataset = Dataset.from_list(eval_data)\n",
    "\n",
    "print(f\"\\nDataset columns: {train_dataset.column_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "\n",
    "# Set padding token if not set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"Pad token: {tokenizer.pad_token}\")\n",
    "print(f\"EOS token: {tokenizer.eos_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model loading configuration\n",
    "model_kwargs = {\n",
    "    \"trust_remote_code\": True,\n",
    "    \"torch_dtype\": torch.bfloat16 if CONFIG[\"bf16\"] else torch.float16,\n",
    "    \"device_map\": \"auto\",\n",
    "}\n",
    "\n",
    "# Add flash attention if available\n",
    "if CONFIG[\"use_flash_attention\"]:\n",
    "    try:\n",
    "        model_kwargs[\"attn_implementation\"] = \"flash_attention_2\"\n",
    "        print(\"Using Flash Attention 2\")\n",
    "    except:\n",
    "        print(\"Flash Attention not available, using default\")\n",
    "\n",
    "# For LoRA with 4-bit quantization\n",
    "USE_4BIT = False\n",
    "\n",
    "if TRAINING_MODE == \"lora\" and USE_4BIT:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    model_kwargs[\"quantization_config\"] = bnb_config\n",
    "    print(\"Using 4-bit quantization (QLoRA)\")\n",
    "\n",
    "# Load model\n",
    "print(f\"Loading {BASE_MODEL}...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, **model_kwargs)\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "if CONFIG[\"gradient_checkpointing\"]:\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "print(f\"Model loaded! Parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA if selected\n",
    "if TRAINING_MODE == \"lora\":\n",
    "    print(\"Applying LoRA...\")\n",
    "    \n",
    "    if USE_4BIT:\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=CONFIG[\"lora_r\"],\n",
    "        lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "        lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "else:\n",
    "    print(\"Full fine-tuning mode - all parameters trainable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=CONFIG[\"num_epochs\"],\n",
    "    per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"] if TRAINING_MODE == \"full\" else 2e-4,\n",
    "    warmup_ratio=CONFIG[\"warmup_ratio\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    \n",
    "    # Precision\n",
    "    bf16=CONFIG[\"bf16\"],\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=CONFIG[\"logging_steps\"],\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    \n",
    "    # Saving\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=CONFIG[\"save_steps\"],\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=CONFIG[\"eval_steps\"],\n",
    "    \n",
    "    # Misc\n",
    "    seed=42,\n",
    "    report_to=\"wandb\" if CONFIG[\"use_wandb\"] else \"none\",\n",
    "    run_name=f\"chess-qwen-1.5b-{datetime.now().strftime('%Y%m%d-%H%M')}\",\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    max_seq_length=CONFIG[\"max_seq_length\"],\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized!\")\n",
    "print(f\"Training examples: {len(train_dataset):,}\")\n",
    "print(f\"Eval examples: {len(eval_dataset):,}\")\n",
    "print(f\"Total steps: {len(train_dataset) // (CONFIG['batch_size'] * CONFIG['gradient_accumulation']) * CONFIG['num_epochs']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training!\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "final_model_path = f\"{OUTPUT_DIR}/final\"\n",
    "\n",
    "print(f\"Saving model to {final_model_path}...\")\n",
    "\n",
    "if TRAINING_MODE == \"lora\":\n",
    "    model.save_pretrained(final_model_path)\n",
    "    tokenizer.save_pretrained(final_model_path)\n",
    "    print(\"LoRA adapters saved!\")\n",
    "    \n",
    "    # Merge and save full model\n",
    "    print(\"Merging LoRA into base model...\")\n",
    "    merged_model = model.merge_and_unload()\n",
    "    merged_path = f\"{OUTPUT_DIR}/merged\"\n",
    "    merged_model.save_pretrained(merged_path)\n",
    "    tokenizer.save_pretrained(merged_path)\n",
    "    print(f\"Merged model saved to {merged_path}\")\n",
    "else:\n",
    "    trainer.save_model(final_model_path)\n",
    "    tokenizer.save_pretrained(final_model_path)\n",
    "    print(\"Full model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test\n",
    "def test_model(model, tokenizer, fen, legal_moves):\n",
    "    \"\"\"Test the model on a position.\"\"\"\n",
    "    prompt = f\"\"\"You are an expert chess player. Here is the position in FEN format:\n",
    "{fen}\n",
    "\n",
    "Legal moves: {legal_moves}\n",
    "\n",
    "Select the best move. Keep your thinking to 2 sentences or less, then output your chosen move.\n",
    "Format:\n",
    "<think>brief thinking (2 sentences max)</think>\n",
    "<uci_move>your_move</uci_move>\"\"\"\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.1,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Test on starting position\n",
    "test_fen = \"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\"\n",
    "test_moves = \"e2e4 d2d4 g1f3 b1c3 c2c4 e2e3 g2g3 b2b3 f2f4 a2a3\"\n",
    "\n",
    "print(\"Testing model on starting position...\")\n",
    "print(\"=\"*60)\n",
    "response = test_model(model, tokenizer, test_fen, test_moves)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a tactical position\n",
    "test_fen2 = \"r1bqkb1r/pppp1ppp/2n2n2/4p3/2B1P3/5N2/PPPP1PPP/RNBQK2R w KQkq - 4 4\"\n",
    "test_moves2 = \"d2d3 d2d4 b1c3 c2c3 e1g1 a2a3 h2h3 b2b4\"\n",
    "\n",
    "print(\"Testing on Italian Game position...\")\n",
    "print(\"=\"*60)\n",
    "response = test_model(model, tokenizer, test_fen2, test_moves2)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Upload to Hugging Face (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run to upload to HuggingFace Hub\n",
    "\n",
    "# from huggingface_hub import login\n",
    "# login()  # Enter your HF token\n",
    "\n",
    "# HF_REPO = \"your-username/qwen-chess-1.5b\"\n",
    "# \n",
    "# model.push_to_hub(HF_REPO)\n",
    "# tokenizer.push_to_hub(HF_REPO)\n",
    "# \n",
    "# print(f\"Model uploaded to https://huggingface.co/{HF_REPO}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
