{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chess Model Training - Qwen2.5-1.5B-Instruct\n",
    "\n",
    "Fine-tune Qwen2.5-1.5B-Instruct on ~280K chess positions.\n",
    "\n",
    "**Hardware**: A40/A100 GPU\n",
    "\n",
    "**Training**: Full fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch transformers datasets accelerate bitsandbytes\n",
    "!pip install -q trl peft wandb hf_transfer\n",
    "!pip install -q flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "OUTPUT_DIR = \"./qwen-chess-1.5b-v1\"\n",
    "DATA_PART1 = \"train_data_part1.jsonl\"\n",
    "DATA_PART2 = \"train_data_part2.jsonl\"\n",
    "\n",
    "TRAINING_MODE = \"full\"  # \"full\" or \"lora\"\n",
    "\n",
    "CONFIG = {\n",
    "    \"num_epochs\": 3,\n",
    "    \"batch_size\": 8,\n",
    "    \"gradient_accumulation\": 4,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"warmup_ratio\": 0.03,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"max_seq_length\": 512,\n",
    "    \"lora_r\": 64,\n",
    "    \"lora_alpha\": 128,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"use_flash_attention\": True,\n",
    "    \"bf16\": True,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"logging_steps\": 100,\n",
    "    \"save_steps\": 2000,\n",
    "    \"eval_strategy\": \"no\",\n",
    "    \"dataloader_num_workers\": 4,\n",
    "    \"pin_memory\": True,\n",
    "    \"optim\": \"adamw_torch_fused\",\n",
    "}\n",
    "\n",
    "print(f\"Mode: {TRAINING_MODE}\")\n",
    "print(f\"Effective batch: {CONFIG['batch_size'] * CONFIG['gradient_accumulation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(filepath):\n",
    "    data = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return data\n",
    "\n",
    "print(\"Loading data...\")\n",
    "part1 = load_jsonl(DATA_PART1)\n",
    "part2 = load_jsonl(DATA_PART2)\n",
    "all_data = part1 + part2\n",
    "print(f\"Total: {len(all_data):,}\")\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(all_data)\n",
    "\n",
    "train_size = int(len(all_data) * 0.98)\n",
    "train_data = all_data[:train_size]\n",
    "eval_data = all_data[train_size:]\n",
    "print(f\"Train: {len(train_data):,}, Eval: {len(eval_data):,}\")\n",
    "\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "eval_dataset = Dataset.from_list(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview\n",
    "sample = all_data[0]\n",
    "print(\"USER:\", sample[\"messages\"][0][\"content\"][:300], \"...\")\n",
    "print(\"\\nASSISTANT:\", sample[\"messages\"][1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "print(f\"Tokenizer loaded. Vocab: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\n",
    "    \"trust_remote_code\": True,\n",
    "    \"torch_dtype\": torch.bfloat16,\n",
    "    \"device_map\": \"auto\",\n",
    "}\n",
    "\n",
    "if CONFIG[\"use_flash_attention\"]:\n",
    "    model_kwargs[\"attn_implementation\"] = \"flash_attention_2\"\n",
    "    print(\"Using Flash Attention 2\")\n",
    "\n",
    "print(f\"Loading {BASE_MODEL}...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, **model_kwargs)\n",
    "\n",
    "if CONFIG[\"gradient_checkpointing\"]:\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "if CONFIG[\"use_flash_attention\"]:\n",
    "    model.config._attn_implementation = \"flash_attention_2\"\n",
    "    print(f\"Attention impl: {model.config._attn_implementation}\")\n",
    "\n",
    "print(f\"Model loaded! Params: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAINING_MODE == \"lora\":\n",
    "    print(\"Applying LoRA...\")\n",
    "    lora_config = LoraConfig(\n",
    "        r=CONFIG[\"lora_r\"],\n",
    "        lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "        lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "else:\n",
    "    print(\"Full fine-tuning mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=CONFIG[\"num_epochs\"],\n",
    "    per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"] if TRAINING_MODE == \"full\" else 2e-4,\n",
    "    warmup_ratio=CONFIG[\"warmup_ratio\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    bf16=CONFIG[\"bf16\"],\n",
    "    tf32=True,\n",
    "    logging_steps=CONFIG[\"logging_steps\"],\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=CONFIG[\"save_steps\"],\n",
    "    save_total_limit=3,\n",
    "    eval_strategy=CONFIG[\"eval_strategy\"],\n",
    "    eval_steps=None,\n",
    "    seed=42,\n",
    "    report_to=\"none\",\n",
    "    optim=CONFIG[\"optim\"],\n",
    "    dataloader_num_workers=CONFIG[\"dataloader_num_workers\"],\n",
    "    dataloader_pin_memory=CONFIG[\"pin_memory\"],\n",
    ")\n",
    "print(\"Training args configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(f\"Trainer ready!\")\n",
    "print(f\"Train: {len(train_dataset):,}, Eval: {len(eval_dataset):,}\")\n",
    "steps = len(train_dataset) // (CONFIG['batch_size'] * CONFIG['gradient_accumulation']) * CONFIG['num_epochs']\n",
    "print(f\"Total steps: ~{steps:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_path = f\"{OUTPUT_DIR}/final\"\n",
    "print(f\"Saving to {final_path}...\")\n",
    "\n",
    "if TRAINING_MODE == \"lora\":\n",
    "    model.save_pretrained(final_path)\n",
    "    tokenizer.save_pretrained(final_path)\n",
    "    print(\"Merging LoRA...\")\n",
    "    merged = model.merge_and_unload()\n",
    "    merged.save_pretrained(f\"{OUTPUT_DIR}/merged\")\n",
    "    tokenizer.save_pretrained(f\"{OUTPUT_DIR}/merged\")\n",
    "else:\n",
    "    trainer.save_model(final_path)\n",
    "    tokenizer.save_pretrained(final_path)\n",
    "\n",
    "print(\"Saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, tokenizer, fen, legal_moves):\n",
    "    prompt = f\"\"\"You are an expert chess player. Here is the position in FEN format:\n",
    "{fen}\n",
    "\n",
    "Legal moves: {legal_moves}\n",
    "\n",
    "Select the best move. Keep your thinking to 2 sentences or less, then output your chosen move.\n",
    "Format:\n",
    "<think>brief thinking (2 sentences max)</think>\n",
    "<uci_move>your_move</uci_move>\"\"\"\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.1, do_sample=True, pad_token_id=tokenizer.pad_token_id)\n",
    "    \n",
    "    return tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "# Test\n",
    "fen = \"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\"\n",
    "moves = \"e2e4 d2d4 g1f3 b1c3 c2c4\"\n",
    "print(\"Testing...\")\n",
    "print(test_model(model, tokenizer, fen, moves))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Upload to HuggingFace (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "# login()\n",
    "# model.push_to_hub(\"your-username/qwen-chess-1.5b\")\n",
    "# tokenizer.push_to_hub(\"your-username/qwen-chess-1.5b\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
